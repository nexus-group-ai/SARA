{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM with RAG demo\n",
    "**Note:** This is just some code to help you getting started, but in no way mandatory to use. Feel free to use any other tools, libraries, approaches, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import pickle\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings.base import OpenAIEmbeddings\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.llm import get_llm_client, get_gemini_embeddings_client\n",
    "\n",
    "if not load_dotenv():\n",
    "    raise Exception('Error loading .env file. Make sure to place a valid OPEN_AI_KEY in the .env file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_PATH = os.path.join(\"..\", \"data\", \"metadata.csv\")\n",
    "ARTICLES_CLEAN_DIR = os.path.join(\"..\", \"data\", \"articles_clean\")\n",
    "DB_PATH = os.path.join(\"..\", \"data\", \"db\", \"sample.db\")\n",
    "if not os.path.exists(DB_PATH):\n",
    "    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the metadata df according to your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(METADATA_PATH)\n",
    "metadata[\"published_at\"] = pd.to_datetime(metadata[\"published_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadata = metadata[\n",
    "    # Date\n",
    "    (metadata[\"published_at\"] >= \"2023-01-01\") &\n",
    "    # Word count\n",
    "    (metadata[\"words_count\"] >= 100) &\n",
    "    # Category\n",
    "    (metadata[\"category\"].str.contains(\"Wirtschaft\", case=False))\n",
    "]\n",
    "print(f\"Expected number of articles: {filtered_metadata.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get filtered articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_articles(filtered_metadata, articles_dir):\n",
    "    \"\"\"\n",
    "    Filter articles based on filtered metadata\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    for _, row in filtered_metadata.iterrows():\n",
    "        article_path = os.path.join(articles_dir, row[\"filename\"])\n",
    "        with open(article_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            article = json.load(file)\n",
    "            articles.append(article)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_articles = filter_articles(filtered_metadata, ARTICLES_CLEAN_DIR)\n",
    "print(f\"Number of articles: {len(filtered_articles)}\\n\")\n",
    "print(f\"Sample article metadata:\\n {filtered_metadata.iloc[0]}\\n\")\n",
    "print(f\"Sample article:\\n {filtered_articles[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create simple vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents_from_path(filenames: list[str]) -> [Document]:\n",
    "    documents = []\n",
    "    \n",
    "    for file in filenames:\n",
    "        text = file.get(\"text\", \"\")\n",
    "        documents.append(Document(page_content=text, metadata={\n",
    "            \"title\": file.get(\"title\", \"\"),\n",
    "            \"author\": file.get(\"author\", \"\"),\n",
    "            \"published_at\": file.get(\"published_at\", \"\"),\n",
    "            \"id\": file.get(\"id\", \"\"),\n",
    "        }))\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_documents_from_path(filtered_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, separators=[\"\\n\\n\", \"\\n\"])\n",
    "\n",
    "# Split documents and create vector database\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "embeddings = get_gemini_embeddings_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count build embedding token number\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "build_token_count = sum([len(tokenizer.encode(doc.page_content)) for doc in texts])\n",
    "print(f\"Token count: {build_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the database\n",
    "with open(DB_PATH, \"wb\") as f:\n",
    "    pickle.dump(db.serialize_to_bytes(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create simple RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLM\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.5,  # <-- Gemini Free Tier\n",
    "    check_every_n_seconds=0.1,\n",
    ")\n",
    "\n",
    "llm = get_llm_client(\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.2,\n",
    "    rate_limiter=rate_limiter,\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert assistant. Use only the following retrieved context to answer the question accurately and concisely. \n",
    "If nothing is mentioned in the context, say \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], \n",
    "    template=system_prompt\n",
    ")\n",
    "\n",
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query):\n",
    "    response = retrieval_chain.invoke({\"query\": query})\n",
    "    print(f\"Question: {query}\\nAnswer: {response['result']}\")\n",
    "    print(\"\\nSources: \\n\")\n",
    "    for source in response[\"source_documents\"]:\n",
    "        print(source.metadata)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_question(\"What was the Austrian GDP development in recent decades?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
